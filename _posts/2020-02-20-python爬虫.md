---
title: python爬虫
categories: python
---
网络爬虫

## 1. web请求

最常用的就是requests库，并且也能实现cookie，代理，认证的需求。
```python
import requests
headers= {
    "User-Agent":"Mozilla/5.0 (Linux; U; Android 4.0; en-us; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30",    
}
# 传参数  request会自动urlencode
requests.get(sendMsg_url,headers=headers,params=postData)
z3 = requests.post(url,headers=headers,data={'openid':'FqNXGNTVuV1HMMBHVZd','app_id':'0002000600020024'})
```

在开发中需要指定HTTP头和编码方式是麻烦的。可以根据请求的返回来指定。
```python
>>> r = requests.get('https://www.baidu.com')
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{"type":"User"...'
>>> r.json()
{u'private_gists': 419, u'total_private_repos': 77, ...}
```

如要下载图片，或者存储数据等，可以用scapy框架处理。[具体使用在这里]()

## 2. 数据处理

网页经常需要对xml和html做处理。常见的库为`xml`和`lxml`,`lxml`包含更多的功能，并且可以解析lxml和xml。



读取解析xml、html数据。并得到root element。
```python
from lxml import etree
root = etree.fromstring('')
#ElementTree
tree = etree.parse('filePath')
root = tree.getroot()
# html
from lxml import etree
html = etree.HTML(data.text)
# 返回满足条件的list
d = html.xpath('xpath')
```

### element

每个标签对应element结构。

`ele.tag` 得到标签名。`ele.text`得到标签内容。`ele.attrib['name']`  得到属性(也可以用xpath得到属性)。


```python
for child in root.getchildren():
    if child.tag == 'struct':
        child.get('desc')
```

通过getchildren方法，或者直接迭代，得到子element。

```python
for child in root.getchildren():
    pass
for child in root:
    pass
```



### xpath

XPath 使用路径表达式在 XML 文档中选取节点。在chrome浏览器调试工具中，可以copy指定element的xpath。
还可以在chrome console控制台验证xpath`$x("//img")`



- 下面列出了最有用的路径表达式

|表达式|	描述|
|--|--|
nodename|选取此节点的所有子节点。
/	|从根节点选取。
//	|从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。
.	|选取当前节点。
..	|选取当前节点的父节点。
@	|选取属性。

- 示例

|示例|描述|
--|--
td/a/@href	|td的所有节点，a的href属性
//title[@lang='eng']	|选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。
//span[@class='red']/text()	|同上，但是不是选取整个标签，而只是内容
.//td	|选了后，再选择子节点

### json处理

```python
import json
string = json.dumps(map)
json.loads('')
json.load(file)
```
### csv

```python
import csv
```
## 3. 自动登陆

通过在请求中附带登陆的cookies达到自动登陆的目的
```python
session = requests.session()
session.cookies = equests.utils.cookiejar_from_dict()
```

1. 在chrome开发工具中，复制请求信息中的cookie信息。
2. 如何在代码中过去cookie信息，还没找到方法

## 5. 最后的办法

对于复杂的页面，复杂的javascript，很难提取出请求的地址和参数。那么可以用`selenium`或`splinter`来完整的模拟浏览器的操作。来进行页面的请求。因为这是完整的模拟所以也会包括前端的渲染，所以性能是最低的。

模拟浏览器需要下载浏览器driver,并且可能需要在Path中指定driver的位置。[chromedriver的下载地址](https://chromedriver.chromium.org/)


## 参考

> Avoiding getting banned：https://docs.scrapy.org/en/latest/topics/practices.html